{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom \"Claudinho\" Wake Word Model\n",
    "\n",
    "Trains an openWakeWord-compatible model using **real voice recordings**.\n",
    "\n",
    "**Runtime: GPU** (Runtime > Change runtime type > T4 GPU)\n",
    "\n",
    "Total time: ~15-20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. SETUP ===\n",
    "!pip install -q openwakeword onnxruntime numpy scipy torch\n",
    "\n",
    "# Get the voice recordings\n",
    "!git clone -q https://github.com/claudinhocoding/claudinho.git claudinho_repo\n",
    "\n",
    "# Download pre-computed negative features\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "\n",
    "# Download embedding models\n",
    "import openwakeword, os\n",
    "oww_dir = os.path.join(os.path.dirname(openwakeword.__file__), 'resources', 'models')\n",
    "os.makedirs(oww_dir, exist_ok=True)\n",
    "!wget -q https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O {oww_dir}/melspectrogram.onnx\n",
    "!wget -q https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O {oww_dir}/embedding_model.onnx\n",
    "\n",
    "import glob\n",
    "samples = sorted(glob.glob('claudinho_repo/training/positive/claudinho/*.wav'))\n",
    "print(f'\\n=== Setup complete: {len(samples)} voice samples found ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. EXTRACT FEATURES FROM RECORDINGS ===\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import scipy.signal\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "mel_session = ort.InferenceSession(os.path.join(oww_dir, 'melspectrogram.onnx'))\n",
    "emb_session = ort.InferenceSession(os.path.join(oww_dir, 'embedding_model.onnx'))\n",
    "\n",
    "WINDOW = 1280      # 80ms at 16kHz\n",
    "N_FRAMES = 76      # embedding model needs 76 mel frames\n",
    "MIN_SAMPLES = N_FRAMES * WINDOW  # ~6.08s\n",
    "\n",
    "def audio_to_features(audio):\n",
    "    \"\"\"Extract openWakeWord embeddings from 16kHz audio. Pads short clips.\"\"\"\n",
    "    audio = np.asarray(audio, dtype=np.float32)\n",
    "    if np.abs(audio).max() > 1.5:  # likely int16\n",
    "        audio = audio / 32768.0\n",
    "\n",
    "    # Pad short clips with silence (centered)\n",
    "    if len(audio) < MIN_SAMPLES:\n",
    "        pad = MIN_SAMPLES - len(audio)\n",
    "        audio = np.pad(audio, (pad // 2, pad - pad // 2))\n",
    "\n",
    "    # Mel spectrograms\n",
    "    mels = []\n",
    "    for s in range(0, len(audio) - WINDOW + 1, WINDOW):\n",
    "        m = mel_session.run(None, {'input': audio[s:s+WINDOW].reshape(1,-1)})[0]\n",
    "        mels.append(m)\n",
    "\n",
    "    # Embeddings (sliding window of 76 mels, step 8)\n",
    "    feats = []\n",
    "    for i in range(0, max(1, len(mels) - N_FRAMES + 1), 8):\n",
    "        if i + N_FRAMES > len(mels): break\n",
    "        batch = np.concatenate(mels[i:i+N_FRAMES]).reshape(1, N_FRAMES, 32)\n",
    "        emb = emb_session.run(None, {'input': batch.astype(np.float32)})[0]\n",
    "        feats.append(emb.flatten())\n",
    "    return np.array(feats) if feats else np.empty((0, 96))\n",
    "\n",
    "def augment(audio):\n",
    "    \"\"\"Generate augmented versions: noise, volume, speed, shift.\"\"\"\n",
    "    a = audio.astype(np.float32)\n",
    "    versions = [a]\n",
    "    # Noise\n",
    "    for nl in [0.003, 0.008, 0.015, 0.025]:\n",
    "        versions.append(a + np.random.randn(len(a)) * nl * 32768)\n",
    "    # Volume\n",
    "    for g in [0.4, 0.6, 0.8, 1.2, 1.5, 2.0]:\n",
    "        versions.append(a * g)\n",
    "    # Time shift\n",
    "    for sh in [1600, 3200, 6400]:\n",
    "        versions.append(np.concatenate([np.zeros(sh), a]))\n",
    "        versions.append(np.concatenate([a, np.zeros(sh)]))\n",
    "    # Speed\n",
    "    for rate in [0.85, 0.92, 1.08, 1.15]:\n",
    "        n = int(len(a) / rate)\n",
    "        versions.append(np.interp(np.linspace(0, len(a)-1, n), np.arange(len(a)), a))\n",
    "    # Noise + volume combos\n",
    "    for nl, g in [(0.005, 0.7), (0.01, 1.3), (0.02, 0.5)]:\n",
    "        versions.append((a + np.random.randn(len(a)) * nl * 32768) * g)\n",
    "    return versions  # ~24 versions per sample\n",
    "\n",
    "print('Extracting features from recordings (with augmentation)...')\n",
    "all_feats = []\n",
    "for i, wav in enumerate(samples):\n",
    "    sr, audio = scipy.io.wavfile.read(wav)\n",
    "    if sr != 16000:\n",
    "        audio = scipy.signal.resample(audio, int(len(audio) * 16000 / sr)).astype(np.int16)\n",
    "    for aug in augment(audio):\n",
    "        f = audio_to_features(aug)\n",
    "        if len(f) > 0:\n",
    "            all_feats.append(f)\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f'  processed {i+1}/{len(samples)} recordings...')\n",
    "\n",
    "pos_features = np.concatenate(all_feats)\n",
    "print(f'\\n=== Positive: {pos_features.shape[0]} embeddings from {len(samples)} recordings ===')\n",
    "print(f'    ({pos_features.shape[0] / len(samples):.0f}x augmentation multiplier)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. LOAD NEGATIVE FEATURES ===\n",
    "print('Loading negative features...')\n",
    "neg_raw = np.load('openwakeword_features_ACAV100M_2000_hrs_16bit.npy')\n",
    "neg_features = neg_raw.astype(np.float32)\n",
    "if neg_raw.dtype == np.int16:\n",
    "    neg_features = neg_features / 256.0\n",
    "\n",
    "# Subsample: 50:1 negative:positive ratio\n",
    "max_neg = min(len(neg_features), pos_features.shape[0] * 50)\n",
    "idx = np.random.choice(len(neg_features), max_neg, replace=False)\n",
    "neg_features = neg_features[idx]\n",
    "\n",
    "val_raw = np.load('validation_set_features.npy')\n",
    "val_neg = val_raw.astype(np.float32)\n",
    "if val_raw.dtype == np.int16:\n",
    "    val_neg = val_neg / 256.0\n",
    "\n",
    "print(f'=== Negative: {neg_features.shape[0]} train, {val_neg.shape[0]} validation ===')\n",
    "print(f'    Ratio: {neg_features.shape[0] / pos_features.shape[0]:.0f}:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. TRAIN THE MODEL ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Normalize features\n",
    "all_X = np.concatenate([pos_features, neg_features])\n",
    "feat_mean = all_X.mean(axis=0)\n",
    "feat_std = all_X.std(axis=0) + 1e-8\n",
    "\n",
    "X_pos = (pos_features - feat_mean) / feat_std\n",
    "X_neg = (neg_features - feat_mean) / feat_std\n",
    "\n",
    "# Train/val split for positives\n",
    "n_val = max(int(len(X_pos) * 0.15), 10)\n",
    "perm = np.random.permutation(len(X_pos))\n",
    "X_pos_val, X_pos_train = X_pos[perm[:n_val]], X_pos[perm[n_val:]]\n",
    "\n",
    "# Assemble training set\n",
    "X_train = np.concatenate([X_pos_train, X_neg])\n",
    "y_train = np.concatenate([np.ones(len(X_pos_train)), np.zeros(len(X_neg))])\n",
    "shuf = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train[shuf], y_train[shuf]\n",
    "\n",
    "# Validation set\n",
    "val_neg_norm = ((val_neg - feat_mean) / feat_std)[:2000]\n",
    "X_val = np.concatenate([X_pos_val, val_neg_norm])\n",
    "y_val = np.concatenate([np.ones(len(X_pos_val)), np.zeros(len(val_neg_norm))])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),\n",
    "    batch_size=512, shuffle=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Train: {len(X_pos_train)} pos + {len(X_neg)} neg')\n",
    "print(f'Val:   {len(X_pos_val)} pos + {len(val_neg_norm)} neg\\n')\n",
    "\n",
    "# Model: simple 2-layer DNN (matches openWakeWord architecture)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(96, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ").to(device)\n",
    "\n",
    "pos_weight = torch.tensor([len(X_neg) / max(len(X_pos_train), 1)]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=75)\n",
    "\n",
    "best_val_acc, best_state = 0, None\n",
    "for epoch in range(75):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb).squeeze(), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vo = model(torch.FloatTensor(X_val).to(device)).squeeze()\n",
    "        vp = (torch.sigmoid(vo) > 0.5).cpu().numpy()\n",
    "        acc = (vp == y_val).mean()\n",
    "        recall = vp[y_val == 1].mean() if (y_val == 1).sum() > 0 else 0\n",
    "        fpr = vp[y_val == 0].mean() if (y_val == 0).sum() > 0 else 0\n",
    "\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1:3d} | Loss {total_loss/len(train_loader):.4f} | Acc {acc:.3f} | Recall {recall:.3f} | FPR {fpr:.4f}')\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f'\\n=== Training done! Best val accuracy: {best_val_acc:.3f} ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. EXPORT TO ONNX ===\n",
    "\n",
    "class ExportModel(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.base(x))\n",
    "\n",
    "export_model = ExportModel(model.cpu()).eval()\n",
    "dummy = torch.randn(1, 96)\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "onnx_path = 'output/claudinho.onnx'\n",
    "\n",
    "torch.onnx.export(\n",
    "    export_model, dummy, onnx_path,\n",
    "    input_names=['input'], output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
    "    opset_version=11,\n",
    ")\n",
    "\n",
    "# Save normalization params\n",
    "np.savez('output/claudinho_norm.npz', mean=feat_mean, std=feat_std)\n",
    "\n",
    "# Verify\n",
    "test_sess = ort.InferenceSession(onnx_path)\n",
    "test_out = test_sess.run(None, {'input': dummy.numpy()})\n",
    "print(f'Model: {os.path.getsize(onnx_path)/1024:.1f} KB')\n",
    "print(f'Output shape: {test_out[0].shape}, test score: {test_out[0][0][0]:.3f}')\n",
    "\n",
    "# Quick accuracy check\n",
    "print('\\nTesting on real samples:')\n",
    "for wav in samples[:5]:\n",
    "    sr, audio = scipy.io.wavfile.read(wav)\n",
    "    f = audio_to_features(audio)\n",
    "    if len(f) > 0:\n",
    "        fn = (f - feat_mean) / feat_std\n",
    "        s = test_sess.run(None, {'input': fn.astype(np.float32)})[0]\n",
    "        print(f'  {os.path.basename(wav)}: {s.max():.3f} {\"OK\" if s.max() > 0.5 else \"LOW\"}')\n",
    "\n",
    "print('\\nNoise (should be low):')\n",
    "for i in range(3):\n",
    "    noise = np.random.randn(48000).astype(np.float32) * 0.1\n",
    "    f = audio_to_features(noise)\n",
    "    if len(f) > 0:\n",
    "        fn = (f - feat_mean) / feat_std\n",
    "        s = test_sess.run(None, {'input': fn.astype(np.float32)})[0]\n",
    "        print(f'  noise_{i}: {s.max():.3f}')\n",
    "\n",
    "print('\\n=== Export complete ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. DOWNLOAD ===\n",
    "from google.colab import files\n",
    "files.download('output/claudinho.onnx')\n",
    "files.download('output/claudinho_norm.npz')\n",
    "print('\\nDone! Copy both files to Pi: ~/claudinho/models/')"
   ]
  }
 ],
 "metadata": {
  "colab": { "provenance": [] },
  "kernelspec": { "display_name": "Python 3", "name": "python3" },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
