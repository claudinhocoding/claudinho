{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom \"Claudinho\" Wake Word Model\n",
    "\n",
    "Trains an openWakeWord-compatible model using **real voice recordings**.\n",
    "\n",
    "Bypasses the complex `train.py` pipeline â€” directly extracts embeddings and trains a classifier.\n",
    "\n",
    "**Runtime: GPU** (Runtime â†’ Change runtime type â†’ T4 GPU)\n",
    "\n",
    "Total time: ~15-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openwakeword onnxruntime numpy scipy\n",
    "\n",
    "# Get the voice recordings\n",
    "!git clone -q https://github.com/claudinhocoding/claudinho.git claudinho_repo\n",
    "\n",
    "# Download pre-computed negative features (~2000 hrs of speech/noise/music)\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "# Validation negative features (~11 hrs)\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "\n",
    "# Download openWakeWord embedding models\n",
    "import openwakeword\n",
    "openwakeword.utils.download_models()\n",
    "\n",
    "import glob\n",
    "samples = sorted(glob.glob('claudinho_repo/training/positive/claudinho/*.wav'))\n",
    "print(f'\\nâœ… Setup complete â€” {len(samples)} voice samples found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Embeddings from Recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Load openWakeWord's feature extraction models\n",
    "oww_dir = Path(openwakeword.__file__).parent / 'resources' / 'models'\n",
    "mel_session = ort.InferenceSession(str(oww_dir / 'melspectrogram.onnx'))\n",
    "emb_session = ort.InferenceSession(str(oww_dir / 'embedding_model.onnx'))\n",
    "\n",
    "def audio_to_features(audio_16k):\n",
    "    \"\"\"\n",
    "    Convert 16kHz audio to openWakeWord embedding features.\n",
    "    Returns array of shape (N, 96) where N = number of 80ms windows.\n",
    "    \"\"\"\n",
    "    # Ensure float32 normalized to [-1, 1]\n",
    "    if audio_16k.dtype == np.int16:\n",
    "        audio_16k = audio_16k.astype(np.float32) / 32768.0\n",
    "    \n",
    "    features = []\n",
    "    # Process in 80ms windows (1280 samples at 16kHz)\n",
    "    window_size = 1280\n",
    "    # Need 76 windows for one embedding (about 6 seconds)\n",
    "    n_windows = 76\n",
    "    \n",
    "    # Collect mel spectrograms\n",
    "    mels = []\n",
    "    for start in range(0, len(audio_16k) - window_size + 1, window_size):\n",
    "        chunk = audio_16k[start:start + window_size]\n",
    "        mel = mel_session.run(None, {'input': chunk.reshape(1, -1)})[0]\n",
    "        mels.append(mel)\n",
    "    \n",
    "    # Convert mel windows to embeddings (76 mels -> 1 embedding)\n",
    "    if len(mels) >= n_windows:\n",
    "        for i in range(0, len(mels) - n_windows + 1, 8):  # step by 8 windows\n",
    "            mel_batch = np.concatenate(mels[i:i + n_windows], axis=0)\n",
    "            mel_batch = mel_batch.reshape(1, n_windows, 32)  # (1, 76, 32)\n",
    "            embedding = emb_session.run(None, {'input': mel_batch.astype(np.float32)})[0]\n",
    "            features.append(embedding.flatten())\n",
    "    \n",
    "    return np.array(features) if features else np.empty((0, 96))\n",
    "\n",
    "\n",
    "def augment_audio(audio, sr=16000):\n",
    "    \"\"\"Simple augmentations: noise, volume, time shift.\"\"\"\n",
    "    augmented = []\n",
    "    audio_f = audio.astype(np.float32)\n",
    "    \n",
    "    # Original\n",
    "    augmented.append(audio_f)\n",
    "    \n",
    "    # Add noise (various levels)\n",
    "    for noise_level in [0.002, 0.005, 0.01, 0.02]:\n",
    "        noisy = audio_f + np.random.randn(len(audio_f)) * noise_level * 32768\n",
    "        augmented.append(noisy)\n",
    "    \n",
    "    # Volume changes\n",
    "    for gain in [0.5, 0.7, 1.3, 1.5]:\n",
    "        augmented.append(audio_f * gain)\n",
    "    \n",
    "    # Time shift (pad start/end)\n",
    "    for shift in [1600, 3200, 4800]:  # 100ms, 200ms, 300ms\n",
    "        shifted = np.concatenate([np.zeros(shift), audio_f])\n",
    "        augmented.append(shifted)\n",
    "        shifted_back = np.concatenate([audio_f, np.zeros(shift)])\n",
    "        augmented.append(shifted_back)\n",
    "    \n",
    "    # Speed changes (slight)\n",
    "    for rate in [0.9, 0.95, 1.05, 1.1]:\n",
    "        new_len = int(len(audio_f) / rate)\n",
    "        resampled = np.interp(\n",
    "            np.linspace(0, len(audio_f) - 1, new_len),\n",
    "            np.arange(len(audio_f)),\n",
    "            audio_f\n",
    "        )\n",
    "        augmented.append(resampled)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "\n",
    "print('Extracting features from recordings...')\n",
    "\n",
    "all_positive_features = []\n",
    "for wav_path in samples:\n",
    "    sr, audio = scipy.io.wavfile.read(wav_path)\n",
    "    if sr != 16000:\n",
    "        # Resample if needed\n",
    "        audio = scipy.signal.resample(audio, int(len(audio) * 16000 / sr)).astype(np.int16)\n",
    "    \n",
    "    # Get features from original + augmented versions\n",
    "    augmented_versions = augment_audio(audio)\n",
    "    for aug_audio in augmented_versions:\n",
    "        feats = audio_to_features(aug_audio.astype(np.float32) / 32768.0)\n",
    "        if len(feats) > 0:\n",
    "            all_positive_features.append(feats)\n",
    "\n",
    "positive_features = np.concatenate(all_positive_features, axis=0)\n",
    "print(f'\\nâœ… Positive features: {positive_features.shape[0]} embeddings from {len(samples)} recordings')\n",
    "print(f'   ({positive_features.shape[0] / len(samples):.0f}x multiplication via augmentation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed negative features\n",
    "print('Loading negative features...')\n",
    "negative_features_raw = np.load('openwakeword_features_ACAV100M_2000_hrs_16bit.npy')\n",
    "\n",
    "# These are stored as int16 to save space, convert to float32\n",
    "if negative_features_raw.dtype == np.int16:\n",
    "    negative_features = negative_features_raw.astype(np.float32) / 256.0  # stored with 8-bit fractional\n",
    "else:\n",
    "    negative_features = negative_features_raw.astype(np.float32)\n",
    "\n",
    "# Subsample negatives (we don't need all 2000 hours)\n",
    "max_negatives = min(len(negative_features), positive_features.shape[0] * 50)  # 50:1 ratio\n",
    "indices = np.random.choice(len(negative_features), max_negatives, replace=False)\n",
    "negative_features = negative_features[indices]\n",
    "\n",
    "print(f'âœ… Negative features: {negative_features.shape[0]} embeddings')\n",
    "print(f'   Ratio: {negative_features.shape[0] / positive_features.shape[0]:.0f}:1 negative:positive')\n",
    "\n",
    "# Validation negatives\n",
    "val_negative = np.load('validation_set_features.npy')\n",
    "if val_negative.dtype == np.int16:\n",
    "    val_negative = val_negative.astype(np.float32) / 256.0\n",
    "else:\n",
    "    val_negative = val_negative.astype(np.float32)\n",
    "print(f'âœ… Validation negatives: {val_negative.shape[0]} embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Simple DNN classifier (matches openWakeWord architecture)\n",
    "class WakeWordModel(nn.Module):\n",
    "    def __init__(self, input_size=96, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Prepare training data\n",
    "X_pos = positive_features.astype(np.float32)\n",
    "X_neg = negative_features.astype(np.float32)\n",
    "\n",
    "# Normalize features\n",
    "all_X = np.concatenate([X_pos, X_neg], axis=0)\n",
    "feat_mean = all_X.mean(axis=0)\n",
    "feat_std = all_X.std(axis=0) + 1e-8\n",
    "X_pos_norm = (X_pos - feat_mean) / feat_std\n",
    "X_neg_norm = (X_neg - feat_mean) / feat_std\n",
    "\n",
    "# Split positive into train/val\n",
    "n_val = max(int(len(X_pos_norm) * 0.15), 10)\n",
    "perm = np.random.permutation(len(X_pos_norm))\n",
    "X_pos_val = X_pos_norm[perm[:n_val]]\n",
    "X_pos_train = X_pos_norm[perm[n_val:]]\n",
    "\n",
    "# Create training set\n",
    "X_train = np.concatenate([X_pos_train, X_neg_norm])\n",
    "y_train = np.concatenate([np.ones(len(X_pos_train)), np.zeros(len(X_neg_norm))])\n",
    "\n",
    "# Shuffle\n",
    "shuffle_idx = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle_idx]\n",
    "y_train = y_train[shuffle_idx]\n",
    "\n",
    "# Validation set\n",
    "val_neg_norm = (val_negative.astype(np.float32) - feat_mean) / feat_std\n",
    "X_val = np.concatenate([X_pos_val, val_neg_norm[:1000]])\n",
    "y_val = np.concatenate([np.ones(len(X_pos_val)), np.zeros(min(1000, len(val_neg_norm)))])\n",
    "\n",
    "# DataLoaders\n",
    "train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "print(f'Training: {len(X_pos_train)} positive + {len(X_neg_norm)} negative = {len(X_train)} total')\n",
    "print(f'Validation: {len(X_pos_val)} positive + {min(1000, len(val_neg_norm))} negative')\n",
    "\n",
    "# Train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = WakeWordModel(input_size=96, hidden_size=32).to(device)\n",
    "\n",
    "# Use weighted loss (positive class is rare)\n",
    "pos_weight = torch.tensor([len(X_neg_norm) / len(X_pos_train)]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Remove sigmoid for BCEWithLogitsLoss\n",
    "model.layers = nn.Sequential(\n",
    "    nn.Linear(96, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch).squeeze()\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(torch.FloatTensor(X_val).to(device)).squeeze()\n",
    "        val_pred = (torch.sigmoid(val_out) > 0.5).cpu().numpy()\n",
    "        val_acc = (val_pred == y_val).mean()\n",
    "        \n",
    "        # Recall on positives\n",
    "        pos_mask = y_val == 1\n",
    "        recall = val_pred[pos_mask].mean() if pos_mask.sum() > 0 else 0\n",
    "        \n",
    "        # False positive rate\n",
    "        neg_mask = y_val == 0\n",
    "        fpr = val_pred[neg_mask].mean() if neg_mask.sum() > 0 else 0\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1:3d} | Loss: {total_loss/len(train_loader):.4f} | '\n",
    "              f'Val Acc: {val_acc:.3f} | Recall: {recall:.3f} | FPR: {fpr:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_state)\n",
    "print(f'\\nâœ… Training complete! Best validation accuracy: {best_val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Export model to ONNX (compatible with openWakeWord)\n",
    "model.eval()\n",
    "model_cpu = model.cpu()\n",
    "\n",
    "# openWakeWord expects: input shape (batch, 96), output shape (batch, 1) with sigmoid\n",
    "# Add sigmoid back for inference\n",
    "class ExportModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.base(x))\n",
    "\n",
    "export_model = ExportModel(model_cpu)\n",
    "export_model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 96)\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "onnx_path = 'output/claudinho.onnx'\n",
    "\n",
    "torch.onnx.export(\n",
    "    export_model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
    "    opset_version=11,\n",
    ")\n",
    "\n",
    "# Save normalization params (needed for inference)\n",
    "np.savez('output/claudinho_norm.npz', mean=feat_mean, std=feat_std)\n",
    "\n",
    "size_kb = os.path.getsize(onnx_path) / 1024\n",
    "print(f'âœ… Model exported: {onnx_path} ({size_kb:.1f} KB)')\n",
    "\n",
    "# Verify it loads\n",
    "test_session = ort.InferenceSession(onnx_path)\n",
    "test_out = test_session.run(None, {'input': dummy_input.numpy()})\n",
    "print(f'âœ… ONNX verification passed (output shape: {test_out[0].shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: score a few positive samples vs random noise\n",
    "model_session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "print('Testing model on real samples:')\n",
    "for wav_path in samples[:5]:\n",
    "    sr, audio = scipy.io.wavfile.read(wav_path)\n",
    "    feats = audio_to_features(audio.astype(np.float32) / 32768.0)\n",
    "    if len(feats) > 0:\n",
    "        feats_norm = (feats - feat_mean) / feat_std\n",
    "        scores = model_session.run(None, {'input': feats_norm.astype(np.float32)})[0]\n",
    "        max_score = scores.max()\n",
    "        print(f'  {os.path.basename(wav_path)}: score={max_score:.3f} {\"âœ…\" if max_score > 0.5 else \"âŒ\"}')\n",
    "\n",
    "print('\\nTesting on random noise (should be low):')\n",
    "for i in range(3):\n",
    "    noise = np.random.randn(16000 * 3).astype(np.float32) * 0.1\n",
    "    feats = audio_to_features(noise)\n",
    "    if len(feats) > 0:\n",
    "        feats_norm = (feats - feat_mean) / feat_std\n",
    "        scores = model_session.run(None, {'input': feats_norm.astype(np.float32)})[0]\n",
    "        max_score = scores.max()\n",
    "        print(f'  noise_{i}: score={max_score:.3f} {\"âœ…\" if max_score < 0.3 else \"âš ï¸\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model files\n",
    "from google.colab import files\n",
    "\n",
    "files.download('output/claudinho.onnx')\n",
    "files.download('output/claudinho_norm.npz')\n",
    "\n",
    "print('\\nðŸŽ‰ Downloads started!')\n",
    "print('\\nNext steps:')\n",
    "print('  1. scp claudinho.onnx claudinho@claudinho.local:~/claudinho/models/')\n",
    "print('  2. scp claudinho_norm.npz claudinho@claudinho.local:~/claudinho/models/')\n",
    "print('  3. ssh claudinho@claudinho.local')\n",
    "print('  4. cd ~/claudinho && git pull && sudo systemctl restart claudinho')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
