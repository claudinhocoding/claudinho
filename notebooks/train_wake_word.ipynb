{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom \"Claudinho\" Wake Word Model\n",
    "\n",
    "This notebook trains an openWakeWord model using **real voice recordings** instead of synthetic TTS.\n",
    "\n",
    "**Runtime: Use GPU** (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)\n",
    "\n",
    "Total time: ~30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openWakeWord and training dependencies\n",
    "!git clone https://github.com/dscripka/openWakeWord\n",
    "!pip install -e ./openWakeWord\n",
    "\n",
    "# Install piper-sample-generator (for adversarial negative generation)\n",
    "!git clone https://github.com/rhasspy/piper-sample-generator\n",
    "!wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
    "!pip install piper-phonemize\n",
    "!pip install webrtcvad\n",
    "\n",
    "# Training dependencies\n",
    "!pip install mutagen==1.47.0\n",
    "!pip install torchinfo==1.8.0\n",
    "!pip install torchmetrics==1.2.0\n",
    "!pip install speechbrain==0.5.14\n",
    "!pip install audiomentations==0.33.0\n",
    "!pip install torch-audiomentations==0.11.0\n",
    "!pip install acoustics==0.2.6\n",
    "!pip install pronouncing==0.2.0\n",
    "!pip install datasets==2.14.6\n",
    "!pip install deep-phonemizer==0.0.19\n",
    "\n",
    "# Fix torchaudio compatibility (set_audio_backend removed in newer versions)\n",
    "import torchaudio\n",
    "if not hasattr(torchaudio, 'set_audio_backend'):\n",
    "    torchaudio.set_audio_backend = lambda x: None\n",
    "\n",
    "# Also patch the file on disk so subprocess calls work\n",
    "import site, glob as g\n",
    "for f in g.glob(site.getsitepackages()[0] + '/torch_audiomentations/utils/io.py'):\n",
    "    txt = open(f).read()\n",
    "    if 'set_audio_backend' in txt:\n",
    "        txt = txt.replace('torchaudio.set_audio_backend(\"soundfile\")', \n",
    "                          'pass  # patched: set_audio_backend removed in newer torchaudio')\n",
    "        open(f, 'w').write(txt)\n",
    "        print('Patched torch_audiomentations/utils/io.py')\n",
    "\n",
    "# Download openWakeWord embedding models\n",
    "import os\n",
    "os.makedirs('./openWakeWord/openwakeword/resources/models', exist_ok=True)\n",
    "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O ./openWakeWord/openwakeword/resources/models/embedding_model.onnx\n",
    "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite -O ./openWakeWord/openwakeword/resources/models/embedding_model.tflite\n",
    "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O ./openWakeWord/openwakeword/resources/models/melspectrogram.onnx\n",
    "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite -O ./openWakeWord/openwakeword/resources/models/melspectrogram.tflite\n",
    "\n",
    "print('\\n‚úÖ Dependencies installed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Claudinho repo to get the recorded voice samples\n",
    "!git clone https://github.com/claudinhocoding/claudinho.git claudinho_repo\n",
    "\n",
    "import glob\n",
    "samples = sorted(glob.glob('claudinho_repo/training/positive/claudinho/*.wav'))\n",
    "print(f'\\n‚úÖ Found {len(samples)} voice samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Room Impulse Responses (for realistic reverb augmentation)\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "\n",
    "output_dir = './mit_rirs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "rir_dataset = datasets.load_dataset('davidscripka/MIT_environmental_impulse_responses', split='train', streaming=True)\n",
    "\n",
    "for row in tqdm(rir_dataset):\n",
    "    name = row['audio']['path'].split('/')[-1]\n",
    "    scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "print('‚úÖ Room impulse responses downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download background noise (AudioSet + Free Music Archive)\n",
    "from pathlib import Path\n",
    "\n",
    "# AudioSet\n",
    "os.makedirs('audioset', exist_ok=True)\n",
    "os.makedirs('./audioset_16k', exist_ok=True)\n",
    "fname = 'bal_train09.tar'\n",
    "!wget -O audioset/{fname} 'https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/{fname}'\n",
    "!cd audioset && tar -xvf {fname}\n",
    "\n",
    "audioset_dataset = datasets.Dataset.from_dict({'audio': [str(i) for i in Path('audioset/audio').glob('**/*.flac')]})\n",
    "audioset_dataset = audioset_dataset.cast_column('audio', datasets.Audio(sampling_rate=16000))\n",
    "for row in tqdm(audioset_dataset):\n",
    "    name = row['audio']['path'].split('/')[-1].replace('.flac', '.wav')\n",
    "    scipy.io.wavfile.write(os.path.join('./audioset_16k', name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "# Free Music Archive (1 hour)\n",
    "os.makedirs('./fma', exist_ok=True)\n",
    "fma_dataset = datasets.load_dataset('rudraml/fma', name='small', split='train', streaming=True)\n",
    "fma_dataset = iter(fma_dataset.cast_column('audio', datasets.Audio(sampling_rate=16000)))\n",
    "for i in tqdm(range(120)):  # 120 clips √ó 30s = 1 hour\n",
    "    row = next(fma_dataset)\n",
    "    name = row['audio']['path'].split('/')[-1].replace('.mp3', '.wav')\n",
    "    scipy.io.wavfile.write(os.path.join('./fma', name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "print('‚úÖ Background noise downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-computed negative features and validation data\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "\n",
    "print('‚úÖ Negative features downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Real Recordings + Generate Adversarial Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "import wave\n",
    "import struct\n",
    "\n",
    "# Create output directory structure that openWakeWord training expects\n",
    "model_dir = './my_custom_model'\n",
    "os.makedirs(f'{model_dir}/positive', exist_ok=True)\n",
    "os.makedirs(f'{model_dir}/positive_val', exist_ok=True)\n",
    "os.makedirs(f'{model_dir}/negative', exist_ok=True)\n",
    "os.makedirs(f'{model_dir}/negative_val', exist_ok=True)\n",
    "\n",
    "# Split recordings: 80% train, 20% validation\n",
    "random.seed(42)\n",
    "all_samples = sorted(glob.glob('claudinho_repo/training/positive/claudinho/*.wav'))\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "split_idx = int(len(all_samples) * 0.8)\n",
    "train_samples = all_samples[:split_idx]\n",
    "val_samples = all_samples[split_idx:]\n",
    "\n",
    "# Copy to model directory\n",
    "for i, src in enumerate(train_samples):\n",
    "    shutil.copy(src, f'{model_dir}/positive/claudinho_{i:04d}.wav')\n",
    "\n",
    "for i, src in enumerate(val_samples):\n",
    "    shutil.copy(src, f'{model_dir}/positive_val/claudinho_{i:04d}.wav')\n",
    "\n",
    "print(f'‚úÖ Split recordings: {len(train_samples)} train, {len(val_samples)} validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adversarial negatives using the openWakeWord pipeline\n",
    "# These are words that SOUND similar to \"claudinho\" but aren't it\n",
    "# This helps the model learn to discriminate\n",
    "\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "# Load default config\n",
    "config = yaml.load(open('openWakeWord/examples/custom_model.yml', 'r').read(), yaml.Loader)\n",
    "\n",
    "# Customize for our use case\n",
    "config['target_phrase'] = ['claudinho']\n",
    "config['model_name'] = 'claudinho'\n",
    "config['n_samples'] = 500  # only need adversarial negatives (not positives)\n",
    "config['n_samples_val'] = 200\n",
    "config['output_dir'] = model_dir\n",
    "config['piper_sample_generator_path'] = './piper-sample-generator'\n",
    "config['rir_paths'] = ['./mit_rirs']\n",
    "config['background_paths'] = ['./audioset_16k', './fma']\n",
    "config['false_positive_validation_data_path'] = 'validation_set_features.npy'\n",
    "config['feature_data_files'] = {'ACAV100M_sample': 'openwakeword_features_ACAV100M_2000_hrs_16bit.npy'}\n",
    "\n",
    "# Training params optimized for small real dataset\n",
    "config['steps'] = 25000\n",
    "config['augmentation_rounds'] = 10  # multiply 53 recordings into ~530 augmented clips\n",
    "config['target_accuracy'] = 0.6\n",
    "config['target_recall'] = 0.3\n",
    "config['target_false_positives_per_hour'] = 0.5\n",
    "config['layer_size'] = 32\n",
    "config['model_type'] = 'dnn'\n",
    "\n",
    "# Custom adversarial phrases (words that might sound like \"claudinho\")\n",
    "config['custom_negative_phrases'] = [\n",
    "    'cloud', 'cloudy', 'clothing', 'clapping', 'climbing',\n",
    "    'cleaning', 'clown', 'clone', 'close', 'club',\n",
    "    'calling', 'coming', 'coding', 'counting', 'cooling'\n",
    "]\n",
    "\n",
    "with open('claudinho_config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print('‚úÖ Training config written')\n",
    "print(f'   Augmentation rounds: {config[\"augmentation_rounds\"]}')\n",
    "print(f'   Training steps: {config[\"steps\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ONLY the adversarial negatives (we already have real positives)\n",
    "# This uses Piper TTS to create English words that sound similar\n",
    "!{sys.executable} openWakeWord/openwakeword/train.py --training_config claudinho_config.yaml --generate_clips\n",
    "\n",
    "print('\\n‚úÖ Adversarial negatives generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Augment and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment clips (adds noise, reverb, volume variation to all clips)\n",
    "# With augmentation_rounds=10, our ~53 train recordings become ~530 unique augmented clips\n",
    "!{sys.executable} openWakeWord/openwakeword/train.py --training_config claudinho_config.yaml --augment_clips\n",
    "\n",
    "print('\\n‚úÖ Augmentation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model! This is the main training step (~15-25 min on T4 GPU)\n",
    "!{sys.executable} openWakeWord/openwakeword/train.py --training_config claudinho_config.yaml --train_model\n",
    "\n",
    "print('\\n‚úÖ Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output\n",
    "model_files = glob.glob(f'{model_dir}/*.onnx') + glob.glob(f'{model_dir}/*.tflite')\n",
    "print('Generated model files:')\n",
    "for f in model_files:\n",
    "    size = os.path.getsize(f) / 1024\n",
    "    print(f'  {f} ({size:.1f} KB)')\n",
    "\n",
    "# Download the .onnx file\n",
    "from google.colab import files\n",
    "onnx_files = glob.glob(f'{model_dir}/*.onnx')\n",
    "if onnx_files:\n",
    "    files.download(onnx_files[0])\n",
    "    print(f'\\nüéâ Download started: {onnx_files[0]}')\n",
    "    print('\\nNext steps:')\n",
    "    print('  1. Copy to Pi: scp claudinho.onnx claudinho@claudinho.local:~/claudinho/models/')\n",
    "    print('  2. Update config.py: WAKE_WORD_MODEL = Path.home() / \"claudinho\" / \"models\" / \"claudinho.onnx\"')\n",
    "    print('  3. Restart service: sudo systemctl restart claudinho')\n",
    "else:\n",
    "    print('‚ùå No .onnx file found ‚Äî check training output above for errors')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
