{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom \"Claudinho\" Wake Word Model\n",
    "\n",
    "**Runtime: GPU** (Runtime > Change runtime type > T4 GPU) then **Runtime > Run all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. SETUP ===\n",
    "!pip install -q openwakeword onnxruntime numpy scipy torch\n",
    "!git clone -q https://github.com/claudinhocoding/claudinho.git claudinho_repo\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "\n",
    "import openwakeword, os, glob\n",
    "oww_dir = os.path.join(os.path.dirname(openwakeword.__file__), 'resources', 'models')\n",
    "os.makedirs(oww_dir, exist_ok=True)\n",
    "!wget -q https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O {oww_dir}/melspectrogram.onnx\n",
    "!wget -q https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O {oww_dir}/embedding_model.onnx\n",
    "\n",
    "samples = sorted(glob.glob('claudinho_repo/training/positive/claudinho/*.wav'))\n",
    "print(f'Setup complete: {len(samples)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. DETECT MODEL SHAPES ===\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import scipy.signal\n",
    "import onnxruntime as ort\n",
    "\n",
    "mel_sess = ort.InferenceSession(os.path.join(oww_dir, 'melspectrogram.onnx'))\n",
    "emb_sess = ort.InferenceSession(os.path.join(oww_dir, 'embedding_model.onnx'))\n",
    "\n",
    "# Get actual input/output names\n",
    "MEL_INPUT_NAME = mel_sess.get_inputs()[0].name\n",
    "EMB_INPUT_NAME = emb_sess.get_inputs()[0].name\n",
    "print(f'Mel input name: {MEL_INPUT_NAME}')\n",
    "print(f'Emb input name: {EMB_INPUT_NAME}')\n",
    "\n",
    "# Probe mel output shape\n",
    "test_mel = mel_sess.run(None, {MEL_INPUT_NAME: np.zeros((1, 1280), dtype=np.float32)})[0]\n",
    "print(f'Mel output shape per 80ms chunk: {test_mel.shape}')\n",
    "\n",
    "# Probe embedding input shape\n",
    "emb_input_info = emb_sess.get_inputs()[0]\n",
    "emb_output_info = emb_sess.get_outputs()[0]\n",
    "print(f'Embedding expects: {emb_input_info.shape}')\n",
    "print(f'Embedding outputs: {emb_output_info.shape}')\n",
    "\n",
    "# Extract the expected shape\n",
    "EMB_INPUT_SHAPE = emb_input_info.shape  # e.g. [1, 76, 32]\n",
    "N_MEL_FRAMES = EMB_INPUT_SHAPE[1]  # how many mel frames the embedding needs\n",
    "MEL_DIM = EMB_INPUT_SHAPE[2]       # mel feature dimension\n",
    "\n",
    "# How many mel frames does one 1280-sample chunk produce?\n",
    "MEL_FRAMES_PER_CHUNK = test_mel.shape[0] if len(test_mel.shape) == 2 else test_mel.shape[1]\n",
    "# If mel output is (1, N, D), flatten to (N, D)\n",
    "test_mel_flat = test_mel.reshape(-1, test_mel.shape[-1]) if len(test_mel.shape) == 3 else test_mel\n",
    "MELS_PER_CHUNK = test_mel_flat.shape[0]\n",
    "ACTUAL_MEL_DIM = test_mel_flat.shape[1]\n",
    "\n",
    "print(f'\\nMel frames per 80ms chunk: {MELS_PER_CHUNK}')\n",
    "print(f'Mel feature dim: {ACTUAL_MEL_DIM}')\n",
    "print(f'Embedding needs {N_MEL_FRAMES} frames of dim {MEL_DIM}')\n",
    "\n",
    "# Chunks needed for one embedding\n",
    "CHUNKS_FOR_EMB = int(np.ceil(N_MEL_FRAMES / MELS_PER_CHUNK))\n",
    "print(f'Audio chunks needed for one embedding: {CHUNKS_FOR_EMB} ({CHUNKS_FOR_EMB * 0.08:.2f}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. FEATURE EXTRACTION ===\n",
    "WINDOW = 1280  # 80ms at 16kHz\n",
    "\n",
    "def audio_to_features(audio):\n",
    "    \"\"\"Extract embeddings from 16kHz audio.\"\"\"\n",
    "    audio = np.asarray(audio, dtype=np.float32)\n",
    "    if np.abs(audio).max() > 1.5:\n",
    "        audio = audio / 32768.0\n",
    "\n",
    "    # Minimum audio length\n",
    "    min_samples = CHUNKS_FOR_EMB * WINDOW\n",
    "    if len(audio) < min_samples:\n",
    "        pad = min_samples - len(audio)\n",
    "        audio = np.pad(audio, (pad // 2, pad - pad // 2))\n",
    "\n",
    "    # Get all mel frames\n",
    "    all_mels = []\n",
    "    for s in range(0, len(audio) - WINDOW + 1, WINDOW):\n",
    "        chunk = audio[s:s+WINDOW].reshape(1, -1)\n",
    "        m = mel_sess.run(None, {MEL_INPUT_NAME: chunk})[0]\n",
    "        # Flatten to (n_frames, mel_dim)\n",
    "        m_flat = m.reshape(-1, m.shape[-1])\n",
    "        all_mels.append(m_flat)\n",
    "\n",
    "    if not all_mels:\n",
    "        return np.empty((0, 96))\n",
    "\n",
    "    # Stack all mel frames\n",
    "    mel_stack = np.concatenate(all_mels, axis=0)  # (total_frames, mel_dim)\n",
    "\n",
    "    # Sliding window over mel frames to get embeddings\n",
    "    feats = []\n",
    "    step = max(1, MELS_PER_CHUNK)  # step by one chunk worth of frames\n",
    "    for i in range(0, len(mel_stack) - N_MEL_FRAMES + 1, step):\n",
    "        batch = mel_stack[i:i+N_MEL_FRAMES]  # (N_MEL_FRAMES, mel_dim)\n",
    "        # Reshape to what embedding model expects\n",
    "        batch = batch[:N_MEL_FRAMES, :MEL_DIM]  # trim if needed\n",
    "        # Reshape to match model's expected rank (could be 3 or 4)\n",
    "        expected_rank = len(EMB_INPUT_SHAPE)\n",
    "        if expected_rank == 4:\n",
    "            batch = batch.reshape(1, 1, N_MEL_FRAMES, MEL_DIM).astype(np.float32)\n",
    "        else:\n",
    "            batch = batch.reshape(1, N_MEL_FRAMES, MEL_DIM).astype(np.float32)\n",
    "        emb = emb_sess.run(None, {EMB_INPUT_NAME: batch})[0]\n",
    "        feats.append(emb.flatten())\n",
    "\n",
    "    return np.array(feats) if feats else np.empty((0, 96))\n",
    "\n",
    "# Quick test\n",
    "sr, test_audio = scipy.io.wavfile.read(samples[0])\n",
    "test_feats = audio_to_features(test_audio)\n",
    "print(f'Test: {os.path.basename(samples[0])} -> {test_feats.shape[0]} embeddings of dim {test_feats.shape[1] if len(test_feats) > 0 else \"?\"}')\n",
    "\n",
    "EMB_DIM = test_feats.shape[1] if len(test_feats) > 0 else 96\n",
    "print(f'Embedding dimension: {EMB_DIM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. AUGMENT & EXTRACT ALL ===\n",
    "def augment(audio):\n",
    "    a = audio.astype(np.float32)\n",
    "    versions = [a]\n",
    "    for nl in [0.003, 0.008, 0.015, 0.025]:\n",
    "        versions.append(a + np.random.randn(len(a)) * nl * 32768)\n",
    "    for g in [0.4, 0.6, 0.8, 1.2, 1.5, 2.0]:\n",
    "        versions.append(a * g)\n",
    "    for sh in [1600, 3200, 6400]:\n",
    "        versions.append(np.concatenate([np.zeros(sh), a]))\n",
    "        versions.append(np.concatenate([a, np.zeros(sh)]))\n",
    "    for rate in [0.85, 0.92, 1.08, 1.15]:\n",
    "        n = int(len(a) / rate)\n",
    "        versions.append(np.interp(np.linspace(0, len(a)-1, n), np.arange(len(a)), a))\n",
    "    for nl, g in [(0.005, 0.7), (0.01, 1.3), (0.02, 0.5)]:\n",
    "        versions.append((a + np.random.randn(len(a)) * nl * 32768) * g)\n",
    "    return versions\n",
    "\n",
    "print('Extracting features...')\n",
    "all_feats = []\n",
    "for i, wav in enumerate(samples):\n",
    "    sr, audio = scipy.io.wavfile.read(wav)\n",
    "    if sr != 16000:\n",
    "        audio = scipy.signal.resample(audio, int(len(audio) * 16000 / sr)).astype(np.int16)\n",
    "    for aug in augment(audio):\n",
    "        f = audio_to_features(aug)\n",
    "        if len(f) > 0:\n",
    "            all_feats.append(f)\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f'  {i+1}/{len(samples)}...')\n",
    "\n",
    "pos_features = np.concatenate(all_feats)\n",
    "print(f'\\nPositive: {pos_features.shape[0]} embeddings ({pos_features.shape[0]/len(samples):.0f}x multiplier)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. LOAD NEGATIVES ===\n",
    "neg_raw = np.load('openwakeword_features_ACAV100M_2000_hrs_16bit.npy')\n",
    "neg_f = neg_raw.astype(np.float32)\n",
    "if neg_raw.dtype == np.int16: neg_f /= 256.0\n",
    "\n",
    "# Match embedding dimension\n",
    "if neg_f.shape[1] != EMB_DIM:\n",
    "    print(f'Warning: neg features dim {neg_f.shape[1]} != {EMB_DIM}, truncating/padding')\n",
    "    if neg_f.shape[1] > EMB_DIM:\n",
    "        neg_f = neg_f[:, :EMB_DIM]\n",
    "    else:\n",
    "        neg_f = np.pad(neg_f, ((0,0),(0, EMB_DIM - neg_f.shape[1])))\n",
    "\n",
    "max_neg = min(len(neg_f), pos_features.shape[0] * 50)\n",
    "neg_f = neg_f[np.random.choice(len(neg_f), max_neg, replace=False)]\n",
    "\n",
    "val_raw = np.load('validation_set_features.npy')\n",
    "val_neg = val_raw.astype(np.float32)\n",
    "if val_raw.dtype == np.int16: val_neg /= 256.0\n",
    "if val_neg.shape[1] != EMB_DIM:\n",
    "    if val_neg.shape[1] > EMB_DIM: val_neg = val_neg[:, :EMB_DIM]\n",
    "    else: val_neg = np.pad(val_neg, ((0,0),(0, EMB_DIM - val_neg.shape[1])))\n",
    "\n",
    "print(f'Negative: {neg_f.shape[0]} train, {val_neg.shape[0]} val (ratio {neg_f.shape[0]/pos_features.shape[0]:.0f}:1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. TRAIN ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "all_X = np.concatenate([pos_features, neg_f])\n",
    "feat_mean, feat_std = all_X.mean(0), all_X.std(0) + 1e-8\n",
    "X_pos = (pos_features - feat_mean) / feat_std\n",
    "X_neg = (neg_f - feat_mean) / feat_std\n",
    "\n",
    "nv = max(int(len(X_pos) * 0.15), 10)\n",
    "p = np.random.permutation(len(X_pos))\n",
    "X_pv, X_pt = X_pos[p[:nv]], X_pos[p[nv:]]\n",
    "\n",
    "X_tr = np.concatenate([X_pt, X_neg])\n",
    "y_tr = np.concatenate([np.ones(len(X_pt)), np.zeros(len(X_neg))])\n",
    "s = np.random.permutation(len(X_tr))\n",
    "X_tr, y_tr = X_tr[s], y_tr[s]\n",
    "\n",
    "vn = ((val_neg - feat_mean) / feat_std)[:2000]\n",
    "X_val = np.concatenate([X_pv, vn])\n",
    "y_val = np.concatenate([np.ones(len(X_pv)), np.zeros(len(vn))])\n",
    "\n",
    "loader = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr)), batch_size=512, shuffle=True)\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {dev} | Train: {len(X_pt)} pos + {len(X_neg)} neg | Val: {nv} pos + {len(vn)} neg')\n",
    "\n",
    "model = nn.Sequential(nn.Linear(EMB_DIM, 32), nn.ReLU(), nn.Linear(32, 1)).to(dev)\n",
    "pw = torch.tensor([len(X_neg) / max(len(X_pt), 1)]).to(dev)\n",
    "crit = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=75)\n",
    "\n",
    "best_acc, best_st = 0, None\n",
    "for ep in range(75):\n",
    "    model.train()\n",
    "    tl = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(dev), yb.to(dev)\n",
    "        opt.zero_grad()\n",
    "        l = crit(model(xb).squeeze(), yb); l.backward(); opt.step(); tl += l.item()\n",
    "    sched.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vo = model(torch.FloatTensor(X_val).to(dev)).squeeze()\n",
    "        vp = (torch.sigmoid(vo) > 0.5).cpu().numpy()\n",
    "        acc = (vp == y_val).mean()\n",
    "        rec = vp[y_val==1].mean() if (y_val==1).sum() > 0 else 0\n",
    "        fpr = vp[y_val==0].mean() if (y_val==0).sum() > 0 else 0\n",
    "    if acc > best_acc: best_acc = acc; best_st = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    if (ep+1) % 10 == 0: print(f'Ep {ep+1:3d} | Loss {tl/len(loader):.4f} | Acc {acc:.3f} | Rec {rec:.3f} | FPR {fpr:.4f}')\n",
    "\n",
    "model.load_state_dict(best_st)\n",
    "print(f'\\nDone! Best accuracy: {best_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. EXPORT ONNX ===\n",
    "class Exp(nn.Module):\n",
    "    def __init__(self, b): super().__init__(); self.b = b\n",
    "    def forward(self, x): return torch.sigmoid(self.b(x))\n",
    "\n",
    "em = Exp(model.cpu()).eval()\n",
    "os.makedirs('output', exist_ok=True)\n",
    "torch.onnx.export(em, torch.randn(1, EMB_DIM), 'output/claudinho.onnx',\n",
    "    input_names=['input'], output_names=['output'],\n",
    "    dynamic_axes={'input':{0:'b'},'output':{0:'b'}}, opset_version=11)\n",
    "np.savez('output/claudinho_norm.npz', mean=feat_mean, std=feat_std)\n",
    "\n",
    "ts = ort.InferenceSession('output/claudinho.onnx')\n",
    "print(f'Model: {os.path.getsize(\"output/claudinho.onnx\")/1024:.1f} KB')\n",
    "\n",
    "print('\\nReal samples:')\n",
    "for w in samples[:5]:\n",
    "    sr, a = scipy.io.wavfile.read(w)\n",
    "    f = audio_to_features(a)\n",
    "    if len(f)>0:\n",
    "        fn = (f-feat_mean)/feat_std\n",
    "        s = ts.run(None,{'input':fn.astype(np.float32)})[0]\n",
    "        print(f'  {os.path.basename(w)}: {s.max():.3f} {\"OK\" if s.max()>0.5 else \"LOW\"}')\n",
    "\n",
    "print('Noise:')\n",
    "for i in range(3):\n",
    "    n = np.random.randn(48000).astype(np.float32)*0.1\n",
    "    f = audio_to_features(n)\n",
    "    if len(f)>0:\n",
    "        fn = (f-feat_mean)/feat_std\n",
    "        s = ts.run(None,{'input':fn.astype(np.float32)})[0]\n",
    "        print(f'  noise_{i}: {s.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. DOWNLOAD ===\n",
    "from google.colab import files\n",
    "files.download('output/claudinho.onnx')\n",
    "files.download('output/claudinho_norm.npz')\n",
    "print('Copy both to Pi: ~/claudinho/models/')"
   ]
  }
 ],
 "metadata": {
  "colab": {"provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"},
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
