{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom \"Claudinho\" Wake Word Model\n",
    "\n",
    "**Runtime: GPU** (Runtime > Change runtime type > T4 GPU) then **Runtime > Run all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. SETUP ===\n",
    "!pip install -q openwakeword onnxruntime numpy scipy torch\n",
    "!git clone -q https://github.com/claudinhocoding/claudinho.git claudinho_repo\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy\n",
    "!wget -q https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy\n",
    "\n",
    "import openwakeword, os, glob\n",
    "oww_dir = os.path.join(os.path.dirname(openwakeword.__file__), 'resources', 'models')\n",
    "os.makedirs(oww_dir, exist_ok=True)\n",
    "!wget -q https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O {oww_dir}/melspectrogram.onnx\n",
    "!wget -q https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O {oww_dir}/embedding_model.onnx\n",
    "\n",
    "samples = sorted(glob.glob('claudinho_repo/training/positive/claudinho/*.wav'))\n",
    "print(f'Setup complete: {len(samples)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. FEATURE EXTRACTION ===\n",
    "# Shapes (verified):\n",
    "#   Mel: input 'input' (1,1280) -> output (1,1,5,32) = 5 frames of 32 mel bands per 80ms\n",
    "#   Emb: input 'input_1' (batch,76,32,1) -> output (batch,1,1,96) = 96-dim embedding\n",
    "#   Need 76 mel frames = ceil(76/5) = 16 chunks = 1.28s of audio\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import scipy.signal\n",
    "import onnxruntime as ort\n",
    "\n",
    "mel_sess = ort.InferenceSession(os.path.join(oww_dir, 'melspectrogram.onnx'))\n",
    "emb_sess = ort.InferenceSession(os.path.join(oww_dir, 'embedding_model.onnx'))\n",
    "\n",
    "WINDOW = 1280          # 80ms at 16kHz\n",
    "MELS_PER_CHUNK = 5     # mel model outputs 5 frames per chunk\n",
    "N_MEL_FRAMES = 76      # embedding model needs 76 mel frames\n",
    "CHUNKS_FOR_EMB = 16    # ceil(76/5) = 16 chunks = 1.28s\n",
    "EMB_DIM = 96\n",
    "\n",
    "def audio_to_features(audio):\n",
    "    \"\"\"Extract 96-dim embeddings from 16kHz audio.\"\"\"\n",
    "    audio = np.asarray(audio, dtype=np.float32)\n",
    "    if np.abs(audio).max() > 1.5:\n",
    "        audio = audio / 32768.0\n",
    "\n",
    "    # Minimum: 16 chunks * 1280 = 20480 samples (1.28s)\n",
    "    min_samples = CHUNKS_FOR_EMB * WINDOW\n",
    "    if len(audio) < min_samples:\n",
    "        pad = min_samples - len(audio)\n",
    "        audio = np.pad(audio, (pad // 2, pad - pad // 2))\n",
    "\n",
    "    # Get mel frames from each 80ms chunk\n",
    "    # Each chunk -> (1,1,5,32), we extract the (5,32) part\n",
    "    all_mel_frames = []\n",
    "    for s in range(0, len(audio) - WINDOW + 1, WINDOW):\n",
    "        chunk = audio[s:s+WINDOW].reshape(1, -1).astype(np.float32)\n",
    "        mel_out = mel_sess.run(None, {'input': chunk})[0]  # (1,1,5,32)\n",
    "        frames = mel_out.reshape(-1, 32)  # (5, 32)\n",
    "        all_mel_frames.append(frames)\n",
    "\n",
    "    if not all_mel_frames:\n",
    "        return np.empty((0, EMB_DIM))\n",
    "\n",
    "    mel_stack = np.concatenate(all_mel_frames, axis=0)  # (total_frames, 32)\n",
    "\n",
    "    # Sliding window: 76 mel frames -> 1 embedding, step by 5 frames (one chunk)\n",
    "    feats = []\n",
    "    for i in range(0, len(mel_stack) - N_MEL_FRAMES + 1, MELS_PER_CHUNK):\n",
    "        batch = mel_stack[i:i+N_MEL_FRAMES]          # (76, 32)\n",
    "        batch = batch.reshape(1, 76, 32, 1).astype(np.float32)  # (1, 76, 32, 1)\n",
    "        emb = emb_sess.run(None, {'input_1': batch})[0]  # (1, 1, 1, 96)\n",
    "        feats.append(emb.flatten())  # (96,)\n",
    "\n",
    "    return np.array(feats) if feats else np.empty((0, EMB_DIM))\n",
    "\n",
    "# Quick test\n",
    "sr, test_audio = scipy.io.wavfile.read(samples[0])\n",
    "test_feats = audio_to_features(test_audio)\n",
    "print(f'Test: {os.path.basename(samples[0])} -> {test_feats.shape} embeddings')\n",
    "print(f'Embedding dim: {test_feats.shape[1] if len(test_feats) > 0 else \"FAILED\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. AUGMENT & EXTRACT ALL ===\n",
    "def augment(audio):\n",
    "    a = audio.astype(np.float32)\n",
    "    v = [a]\n",
    "    for nl in [0.003, 0.008, 0.015, 0.025]:\n",
    "        v.append(a + np.random.randn(len(a)) * nl * 32768)\n",
    "    for g in [0.4, 0.6, 0.8, 1.2, 1.5, 2.0]:\n",
    "        v.append(a * g)\n",
    "    for sh in [1600, 3200, 6400]:\n",
    "        v.append(np.concatenate([np.zeros(sh), a]))\n",
    "        v.append(np.concatenate([a, np.zeros(sh)]))\n",
    "    for rate in [0.85, 0.92, 1.08, 1.15]:\n",
    "        n = int(len(a) / rate)\n",
    "        v.append(np.interp(np.linspace(0, len(a)-1, n), np.arange(len(a)), a))\n",
    "    for nl, g in [(0.005, 0.7), (0.01, 1.3), (0.02, 0.5)]:\n",
    "        v.append((a + np.random.randn(len(a)) * nl * 32768) * g)\n",
    "    return v\n",
    "\n",
    "print('Extracting features with augmentation...')\n",
    "all_feats = []\n",
    "for i, wav in enumerate(samples):\n",
    "    sr, audio = scipy.io.wavfile.read(wav)\n",
    "    if sr != 16000:\n",
    "        audio = scipy.signal.resample(audio, int(len(audio) * 16000 / sr)).astype(np.int16)\n",
    "    for aug in augment(audio):\n",
    "        f = audio_to_features(aug)\n",
    "        if len(f) > 0:\n",
    "            all_feats.append(f)\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f'  {i+1}/{len(samples)}...')\n",
    "\n",
    "pos_features = np.concatenate(all_feats)\n",
    "print(f'\\nPositive: {pos_features.shape[0]} embeddings ({pos_features.shape[0]/len(samples):.0f}x per sample)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. LOAD NEGATIVES (memory-efficient) ===\n",
    "# File is 3D: (N, 16, 96) â€” each row has 16 consecutive 96-dim embeddings\n",
    "neg_mmap = np.load('openwakeword_features_ACAV100M_2000_hrs_16bit.npy', mmap_mode='r')\n",
    "print(f'Negative file: {neg_mmap.shape}, dtype={neg_mmap.dtype}')\n",
    "\n",
    "# Sample rows, then flatten 16 embeddings per row into individual samples\n",
    "n_pos = pos_features.shape[0]\n",
    "rows_needed = min(len(neg_mmap), (n_pos * 20) // 16 + 1)  # 20:1 ratio\n",
    "idx = np.sort(np.random.choice(len(neg_mmap), rows_needed, replace=False))\n",
    "neg_sampled = neg_mmap[idx].astype(np.float32)  # (rows, 16, 96)\n",
    "neg_f = neg_sampled.reshape(-1, EMB_DIM)  # flatten to (rows*16, 96)\n",
    "neg_f = neg_f[:n_pos * 20]  # trim to exact ratio\n",
    "del neg_mmap, neg_sampled\n",
    "\n",
    "# Validation negatives\n",
    "val_mmap = np.load('validation_set_features.npy', mmap_mode='r')\n",
    "print(f'Val file: {val_mmap.shape}, dtype={val_mmap.dtype}')\n",
    "val_neg = val_mmap[:2000].astype(np.float32)\n",
    "if val_neg.ndim == 3:  # (N, 16, 96) -> (N*16, 96)\n",
    "    val_neg = val_neg.reshape(-1, EMB_DIM)\n",
    "del val_mmap\n",
    "\n",
    "print(f'Sampled: {neg_f.shape[0]} train neg, {val_neg.shape[0]} val neg ({neg_f.shape[0]/n_pos:.0f}:1 ratio)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. TRAIN ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "all_X = np.concatenate([pos_features, neg_f])\n",
    "feat_mean, feat_std = all_X.mean(0), all_X.std(0) + 1e-8\n",
    "X_pos = (pos_features - feat_mean) / feat_std\n",
    "X_neg = (neg_f - feat_mean) / feat_std\n",
    "\n",
    "nv = max(int(len(X_pos) * 0.15), 10)\n",
    "p = np.random.permutation(len(X_pos))\n",
    "X_pv, X_pt = X_pos[p[:nv]], X_pos[p[nv:]]\n",
    "\n",
    "X_tr = np.concatenate([X_pt, X_neg])\n",
    "y_tr = np.concatenate([np.ones(len(X_pt)), np.zeros(len(X_neg))])\n",
    "s = np.random.permutation(len(X_tr)); X_tr, y_tr = X_tr[s], y_tr[s]\n",
    "\n",
    "vn = ((val_neg - feat_mean) / feat_std)[:2000]\n",
    "X_val = np.concatenate([X_pv, vn])\n",
    "y_val = np.concatenate([np.ones(len(X_pv)), np.zeros(len(vn))])\n",
    "\n",
    "loader = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr)), batch_size=512, shuffle=True)\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {dev} | Train: {len(X_pt)} pos + {len(X_neg)} neg | Val: {nv} pos + {len(vn)} neg')\n",
    "\n",
    "model = nn.Sequential(nn.Linear(EMB_DIM, 32), nn.ReLU(), nn.Linear(32, 1)).to(dev)\n",
    "pw = torch.tensor([len(X_neg) / max(len(X_pt), 1)]).to(dev)\n",
    "crit = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=75)\n",
    "\n",
    "best_acc, best_st = 0, None\n",
    "for ep in range(75):\n",
    "    model.train(); tl = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(dev), yb.to(dev)\n",
    "        opt.zero_grad(); l = crit(model(xb).squeeze(), yb); l.backward(); opt.step(); tl += l.item()\n",
    "    sched.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vo = model(torch.FloatTensor(X_val).to(dev)).squeeze()\n",
    "        vp = (torch.sigmoid(vo) > 0.5).cpu().numpy()\n",
    "        acc = (vp == y_val).mean()\n",
    "        rec = vp[y_val==1].mean() if (y_val==1).sum() > 0 else 0\n",
    "        fpr = vp[y_val==0].mean() if (y_val==0).sum() > 0 else 0\n",
    "    if acc > best_acc: best_acc = acc; best_st = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    if (ep+1) % 10 == 0: print(f'Ep {ep+1:3d} | Loss {tl/len(loader):.4f} | Acc {acc:.3f} | Rec {rec:.3f} | FPR {fpr:.4f}')\n",
    "\n",
    "model.load_state_dict(best_st)\n",
    "print(f'\\nDone! Best accuracy: {best_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. EXPORT & TEST ===\n",
    "class Exp(nn.Module):\n",
    "    def __init__(self, b): super().__init__(); self.b = b\n",
    "    def forward(self, x): return torch.sigmoid(self.b(x))\n",
    "\n",
    "em = Exp(model.cpu()).eval()\n",
    "os.makedirs('output', exist_ok=True)\n",
    "torch.onnx.export(em, torch.randn(1, EMB_DIM), 'output/claudinho.onnx',\n",
    "    input_names=['input'], output_names=['output'],\n",
    "    dynamic_axes={'input':{0:'b'},'output':{0:'b'}}, opset_version=11)\n",
    "np.savez('output/claudinho_norm.npz', mean=feat_mean, std=feat_std)\n",
    "\n",
    "ts = ort.InferenceSession('output/claudinho.onnx')\n",
    "print(f'Model: {os.path.getsize(\"output/claudinho.onnx\")/1024:.1f} KB')\n",
    "\n",
    "print('\\nPositive samples (should be > 0.5):')\n",
    "for w in samples[:5]:\n",
    "    sr, a = scipy.io.wavfile.read(w)\n",
    "    f = audio_to_features(a)\n",
    "    if len(f) > 0:\n",
    "        fn = (f - feat_mean) / feat_std\n",
    "        s = ts.run(None, {'input': fn.astype(np.float32)})[0]\n",
    "        print(f'  {os.path.basename(w)}: {s.max():.3f} {\"OK\" if s.max() > 0.5 else \"LOW\"}')\n",
    "\n",
    "print('\\nNoise (should be < 0.3):')\n",
    "for i in range(3):\n",
    "    n = np.random.randn(48000).astype(np.float32) * 0.1\n",
    "    f = audio_to_features(n)\n",
    "    if len(f) > 0:\n",
    "        fn = (f - feat_mean) / feat_std\n",
    "        s = ts.run(None, {'input': fn.astype(np.float32)})[0]\n",
    "        print(f'  noise_{i}: {s.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. DOWNLOAD ===\n",
    "from google.colab import files\n",
    "files.download('output/claudinho.onnx')\n",
    "files.download('output/claudinho_norm.npz')\n",
    "print('Copy both to Pi: ~/claudinho/models/')"
   ]
  }
 ],
 "metadata": {
  "colab": {"provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"},
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
